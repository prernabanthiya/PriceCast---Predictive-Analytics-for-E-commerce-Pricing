{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVlYUD2q__pd"
      },
      "source": [
        "# Amazon ML Challenge — Multimodal Price Prediction\n",
        "\n",
        "\n"
      ],
      "id": "bVlYUD2q__pd"
    },
    {
      "cell_type": "code",
      "source": [
        "!fusermount -u /content/drive\n",
        "!rm -rf /content/drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peB-H6D6EO86",
        "outputId": "13233ffe-ee45-4ac1-c664-272c35cad23e"
      },
      "id": "peB-H6D6EO86",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fusermount: failed to unmount /content/drive: No such file or directory\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "DATA_DIR = \"/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/dataset\"\n",
        "print(\"Exists:\", Path(DATA_DIR).exists())\n",
        "print(\"Files:\", [x.name for x in Path(DATA_DIR).iterdir()])\n",
        "\n",
        "import pandas as pd\n",
        "train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
        "test  = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
        "sample_in  = pd.read_csv(f\"{DATA_DIR}/sample_test.csv\")\n",
        "sample_out = pd.read_csv(f\"{DATA_DIR}/sample_test_out.csv\")\n",
        "print(train.shape, test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SneJmDE_EbIL",
        "outputId": "701375bb-a2d4-48dc-8b78-5b120906816c"
      },
      "id": "SneJmDE_EbIL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exists: True\n",
            "Files: ['sample_test_out.csv', 'sample_test.csv', 'test.csv', 'train.csv']\n",
            "(75000, 4) (75000, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install git+https://github.com/openai/CLIP.git\n",
        "!pip -q install open-clip-torch ftfy regex tqdm pillow\n",
        "!pip -q install sentence-transformers scikit-learn joblib\n",
        "!pip -q install requests aiohttp aiofiles tenacity\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXkL7JtAGA92",
        "outputId": "6b542f4e-4285-4482-c156-f3939f5337bb"
      },
      "id": "CXkL7JtAGA92",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxKBLjRu__pk",
        "outputId": "5867cc01-acd0-4239-ae62-21d3afe48608"
      },
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import joblib\n",
        "import torch\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "\n",
        "DATA_DIR = \"/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/dataset\"\n",
        "\n",
        "BASE = \"/content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource\"\n",
        "IMG_DIR = f\"{BASE}/images\"\n",
        "EMB_DIR = f\"{BASE}/embeddings\"\n",
        "SUB_DIR = f\"{BASE}/submissions\"\n",
        "\n",
        "for d in [IMG_DIR, EMB_DIR, SUB_DIR]:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print('Dirs ready:', IMG_DIR, EMB_DIR, SUB_DIR)\n",
        "\n",
        "# Sanity check that CSVs exist where expected\n",
        "from pathlib import Path\n",
        "p = Path(DATA_DIR)\n",
        "print(\"DATA_DIR exists:\", p.exists())\n",
        "print(\"Files:\", [x.name for x in p.iterdir()] if p.exists() else \"Not found\")\n"
      ],
      "id": "nxKBLjRu__pk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dirs ready: /content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/images /content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/embeddings /content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/submissions\n",
            "DATA_DIR exists: True\n",
            "Files: ['sample_test_out.csv', 'sample_test.csv', 'test.csv', 'train.csv']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHEjM8fE__po"
      },
      "source": [
        "train = pd.read_csv(f\"{DATA_DIR}/train.csv\")\n",
        "test  = pd.read_csv(f\"{DATA_DIR}/test.csv\")\n",
        "sample_in  = pd.read_csv(f\"{DATA_DIR}/sample_test.csv\")\n",
        "sample_out = pd.read_csv(f\"{DATA_DIR}/sample_test_out.csv\")\n"
      ],
      "id": "oHEjM8fE__po",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def normalize_text(s):\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    return (s.replace('\\u2019', \"'\")\n",
        "             .replace('\\u2018', \"'\")\n",
        "             .replace('\\u2013', '-')\n",
        "             .replace('\\u2014', '-')\n",
        "             .strip())\n",
        "\n",
        "def parse_pack_count(s):\n",
        "    if not isinstance(s, str):\n",
        "        return 1\n",
        "    s = normalize_text(s)\n",
        "    m = re.search(r'(pack of|per case|case of)\\s*(\\d+)', s, flags=re.I)\n",
        "    if m:\n",
        "        try: return int(m.group(2))\n",
        "        except Exception: pass\n",
        "    m = re.search(r'[\\(\\s]x\\s*(\\d+)[\\)\\s]', s, flags=re.I)\n",
        "    if m:\n",
        "        try: return int(m.group(1))\n",
        "        except Exception: pass\n",
        "    m = re.search(r'(\\d+)[\\-\\s]?pack\\b', s, flags=re.I)\n",
        "    if m:\n",
        "        try: return int(m.group(1))\n",
        "        except Exception: pass\n",
        "    return 1\n",
        "\n",
        "# Additional pack-of patterns like \"12 ct\", \"24 count\", \"6 pcs\"\n",
        "def parse_pack_count_extra(s):\n",
        "    if not isinstance(s, str):\n",
        "        return None\n",
        "    s = normalize_text(s)\n",
        "    m = re.search(r'(\\d+)\\s*(?:ct|count|pcs|pieces)\\b', s, flags=re.I)\n",
        "    if m:\n",
        "        try: return int(m.group(1))\n",
        "        except: pass\n",
        "    # generic \"<num> x\" when not captured by previous regex\n",
        "    m = re.search(r'\\b(\\d+)\\s*x\\b', s, flags=re.I)\n",
        "    if m:\n",
        "        try: return int(m.group(1))\n",
        "        except: pass\n",
        "    return None\n",
        "\n",
        "def parse_size(s):\n",
        "    if not isinstance(s, str):\n",
        "        return np.nan, None\n",
        "    s = normalize_text(s)\n",
        "    m = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(fl\\.?\\s*oz|fluid\\s*ounce|ounce|oz|ml|l|g|kg|lb|lbs)\\b', s, flags=re.I)\n",
        "    if not m:\n",
        "        return np.nan, None\n",
        "    try:\n",
        "        val = float(m.group(1))\n",
        "    except Exception:\n",
        "        return np.nan, None\n",
        "    unit = m.group(2).lower().replace('.', '').replace(' ', '')\n",
        "    return val, unit\n",
        "\n",
        "# Composite qty patterns: \"2x500g\", \"500 g x 2\", \"2 x 500 ml\"\n",
        "_QTY_BLOCKS = [\n",
        "    r'(\\d+(?:\\.\\d+)?)\\s*(ml|l|g|kg|oz|lb|lbs)\\s*x\\s*(\\d+)',\n",
        "    r'(\\d+)\\s*x\\s*(\\d+(?:\\.\\d+)?)\\s*(ml|l|g|kg|oz|lb|lbs)',\n",
        "    r'(\\d+)\\s*x\\s*(\\d+(?:\\.\\d+)?)(ml|l|g|kg|oz|lb|lbs)',\n",
        "    r'(\\d+(?:\\.\\d+)?)\\s*(ml|l|g|kg|oz|lb|lbs)\\s*-\\s*(?:pack of|x)\\s*(\\d+)',\n",
        "    r'(\\d+)\\s*pack\\s*of\\s*(\\d+(?:\\.\\d+)?)\\s*(ml|l|g|kg|oz|lb|lbs)',\n",
        "    r'(\\d+)x(\\d+(?:\\.\\d+)?)(ml|l|g|kg|oz|lb|lbs)'\n",
        "]\n",
        "\n",
        "def parse_composite_qty(s):\n",
        "    if not isinstance(s, str):\n",
        "        return None, None, None\n",
        "    s = normalize_text(s)\n",
        "    for pat in _QTY_BLOCKS:\n",
        "        m = re.search(pat, s, flags=re.I)\n",
        "        if m:\n",
        "            g = [x for x in m.groups() if x is not None]\n",
        "            nums  = [x for x in g if re.match(r'^\\d+(?:\\.\\d+)?$', x)]\n",
        "            units = [x.lower() for x in g if re.match(r'^(ml|l|g|kg|oz|lb|lbs)$', x, flags=re.I)]\n",
        "            if len(nums) >= 2 and len(units) >= 1:\n",
        "                try:\n",
        "                    a, b = float(nums[0]), float(nums[1])  # two numbers found\n",
        "                    # Interpret as count x size\n",
        "                    if a.is_integer():\n",
        "                        count = int(a); size_val = b\n",
        "                    else:\n",
        "                        count = int(b) if b.is_integer() else int(round(b))\n",
        "                        size_val = a\n",
        "                    unit = units[0].replace('.', '')\n",
        "                    return count, size_val, unit\n",
        "                except:\n",
        "                    continue\n",
        "    return None, None, None\n",
        "\n",
        "UNIT_TO_BASE = {\n",
        "    'oz': ('g', 28.3495), 'ounce': ('g', 28.3495),\n",
        "    'floz': ('ml', 29.5735), 'fluidounce': ('ml', 29.5735),\n",
        "    'ml': ('ml', 1.0), 'l': ('ml', 1000.0),\n",
        "    'g': ('g', 1.0), 'kg': ('g', 1000.0),\n",
        "    'lb': ('g', 453.592), 'lbs': ('g', 453.592),\n",
        "}\n",
        "\n",
        "def normalize_qty(v, u):\n",
        "    if u is None or not np.isfinite(v):\n",
        "        return np.nan, None\n",
        "    if u in UNIT_TO_BASE:\n",
        "        base, factor = UNIT_TO_BASE[u]\n",
        "        return v * factor, base\n",
        "    return np.nan, None\n",
        "\n",
        "def brand_guess(s):\n",
        "    if not isinstance(s, str):\n",
        "        return ''\n",
        "    s = normalize_text(s)\n",
        "    m = re.search(r'Item Name:\\s*([^,\\n]+)', s, flags=re.I)\n",
        "    if m:\n",
        "        cand = m.group(1).strip()\n",
        "        if cand:\n",
        "            parts = cand.split()\n",
        "            if len(parts) > 0:\n",
        "                return parts[0]\n",
        "    m2 = re.search(r\"\\b([A-Z][A-Za-z'-]{2,})\\b\", s)\n",
        "    return m2.group(1) if m2 else ''\n",
        "\n",
        "# === Apply base parsing ===\n",
        "for df in [train, test]:\n",
        "    df['catalog_content'] = df['catalog_content'].fillna('').apply(normalize_text)\n",
        "    df['pack_count'] = df['catalog_content'].apply(parse_pack_count)\n",
        "    size_list = df['catalog_content'].apply(parse_size)\n",
        "    df['size_val']  = [v for v, _ in size_list]\n",
        "    df['size_unit'] = [u for _, u in size_list]\n",
        "    norm_list = [normalize_qty(v, u) for v, u in zip(df['size_val'], df['size_unit'])]\n",
        "    df['norm_qty']  = [v for v, _ in norm_list]\n",
        "    df['norm_unit'] = [u for _, u in norm_list]\n",
        "    df['total_qty'] = df['pack_count'] * df['norm_qty'].fillna(0)\n",
        "    df['brand_guess'] = df['catalog_content'].apply(brand_guess)\n",
        "\n",
        "# === Extend with extra parsing and enriched features ===\n",
        "for df in [train, test]:\n",
        "    # Better pack count if missing\n",
        "    pc_extra = df['catalog_content'].apply(parse_pack_count_extra).fillna(0).astype(int)\n",
        "    df['pack_count'] = np.where((df['pack_count']<=1) & (pc_extra>1), pc_extra, df['pack_count'])\n",
        "\n",
        "    # Composite quantity like \"2x500g\"\n",
        "    comp = df['catalog_content'].apply(parse_composite_qty)\n",
        "    comp_count = [c if c is not None else np.nan for c,_,_ in comp]\n",
        "    comp_size  = [v if v is not None else np.nan for _,v,_ in comp]\n",
        "    comp_unit  = [u if u is not None else None for _,_,u in comp]\n",
        "\n",
        "    # Prefer composite when present\n",
        "    df['size_val']  = np.where(np.isfinite(comp_size), comp_size, df['size_val'])\n",
        "    df['size_unit'] = np.where(pd.Series(comp_unit).notna(), comp_unit, df['size_unit'])\n",
        "\n",
        "    # Re-normalize quantities and totals\n",
        "    norm_list2 = [normalize_qty(v, u) for v, u in zip(df['size_val'], df['size_unit'])]\n",
        "    df['norm_qty']  = [v for v,_ in norm_list2]\n",
        "    df['norm_unit'] = [u for _,u in norm_list2]\n",
        "    df['total_qty'] = df['pack_count'] * df['norm_qty'].fillna(0)\n",
        "\n",
        "# === Numeric features list (extended) ===\n",
        "num_cols = ['pack_count', 'size_val', 'norm_qty', 'total_qty']\n",
        "# === Additional text-based features for SMAPE boost ===\n",
        "train['has_sale']      = train['catalog_content'].str.contains('sale|discount', case=False).astype(int)\n",
        "train['text_word_cnt'] = train['catalog_content'].str.split().str.len()\n",
        "train['avg_wlen']      = train['catalog_content'].str.len() / (train['text_word_cnt'] + 1)\n",
        "\n",
        "test['has_sale']       = test['catalog_content'].str.contains('sale|discount', case=False).astype(int)\n",
        "test['text_word_cnt']  = test['catalog_content'].str.split().str.len()\n",
        "test['avg_wlen']       = test['catalog_content'].str.len() / (test['text_word_cnt'] + 1)\n",
        "\n",
        "num_cols += ['has_sale', 'text_word_cnt', 'avg_wlen']\n",
        "# Robust extras used by models\n",
        "train['has_qty']  = (~train['norm_qty'].isna()).astype(int)\n",
        "test['has_qty']   = (~test['norm_qty'].isna()).astype(int)\n",
        "for df in (train, test):\n",
        "    df['log_total_qty'] = np.log1p(df['total_qty'].fillna(0))\n",
        "    df['qty_per_pack']  = np.log1p(df['norm_qty'].fillna(0)/np.maximum(df['pack_count'],1))\n",
        "    df['has_pack']      = (df['pack_count']>1).astype(int)\n",
        "\n",
        "extra_num = ['has_qty','log_total_qty','qty_per_pack','has_pack']\n",
        "num_cols = list(dict.fromkeys(num_cols + extra_num))\n",
        "\n",
        "for c in num_cols:\n",
        "    train[c] = train[c].fillna(0).astype(float)\n",
        "    test[c]  = test[c].fillna(0).astype(float)\n",
        "\n",
        "print('Numeric feature summary:')\n",
        "print(train[num_cols].describe())\n",
        "print('Brand guess examples:', train['brand_guess'].value_counts().head(10).to_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWNzJ4oCUYjG",
        "outputId": "489d5ccf-c1d9-4a28-a468-ed734cd11f9a"
      },
      "id": "cWNzJ4oCUYjG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Numeric feature summary:\n",
            "         pack_count      size_val      norm_qty     total_qty      has_sale  \\\n",
            "count  75000.000000  75000.000000  7.500000e+04  7.500000e+04  75000.000000   \n",
            "mean      13.850880     17.755865  6.512404e+02  9.660751e+03      0.007133   \n",
            "std      509.352042     95.467490  6.397394e+04  1.377993e+06      0.084158   \n",
            "min        0.000000      0.000000  0.000000e+00  0.000000e+00      0.000000   \n",
            "25%        1.000000      0.000000  0.000000e+00  0.000000e+00      0.000000   \n",
            "50%        1.000000      4.250000  1.417475e+02  2.500000e+02      0.000000   \n",
            "75%        6.000000     12.000000  4.036783e+02  9.071840e+02      0.000000   \n",
            "max    93207.000000  17400.000000  1.740000e+07  3.740008e+08      1.000000   \n",
            "\n",
            "       text_word_cnt      avg_wlen       has_qty  log_total_qty  qty_per_pack  \\\n",
            "count   75000.000000  75000.000000  75000.000000   75000.000000  75000.000000   \n",
            "mean      147.851693      5.949507      0.714240       4.481155      3.347319   \n",
            "std       137.068731      0.465673      0.451779       3.160358      2.584223   \n",
            "min         7.000000      3.736842      0.000000       0.000000      0.000000   \n",
            "25%        42.000000      5.694055      0.000000       0.000000      0.000000   \n",
            "50%       104.000000      5.982327      1.000000       5.525453      3.851592   \n",
            "75%       208.000000      6.229730      1.000000       6.811447      5.530915   \n",
            "max      1333.000000     12.020000      1.000000      19.739768     16.671981   \n",
            "\n",
            "           has_pack  \n",
            "count  75000.000000  \n",
            "mean       0.436067  \n",
            "std        0.495899  \n",
            "min        0.000000  \n",
            "25%        0.000000  \n",
            "50%        0.000000  \n",
            "75%        1.000000  \n",
            "max        1.000000  \n",
            "Brand guess examples: {'Food': 984, 'McCormick': 626, 'The': 619, 'Goya': 442, 'Organic': 428, 'Rani': 425, 'Frontier': 335, 'Betty': 325, 'La': 316, 'Starbucks': 304}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETRVYW1g__pq",
        "outputId": "4e7dc281-bfb4-427d-d4b5-d666bb3a1ac3"
      },
      "source": [
        "# Image embedding with CLIP. Uses local images in IMG_DIR named <sample_id>.jpg (or .png)\n",
        "try:\n",
        "    import clip\n",
        "    use_openai_clip = True\n",
        "except Exception:\n",
        "    use_openai_clip = False\n",
        "    try:\n",
        "        import open_clip\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "from PIL import Image\n",
        "import tqdm\n",
        "def load_image_for_sid(sid):\n",
        "    p1 = Path(IMG_DIR)/f\"{sid}.jpg\"\n",
        "    p2 = Path(IMG_DIR)/f\"{sid}.png\"\n",
        "    if p1.exists(): return p1\n",
        "    if p2.exists(): return p2\n",
        "    return None\n",
        "\n",
        "def encode_images(df, out_path):\n",
        "    # returns numpy array of embeddings\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    if use_openai_clip:\n",
        "        model, preprocess = clip.load('ViT-B/32', device=device)\n",
        "        model.eval()\n",
        "        emb_dim = model.visual.output_dim if hasattr(model, 'visual') else 512\n",
        "    else:\n",
        "        import open_clip\n",
        "        model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "        model.to(device); model.eval()\n",
        "        emb_dim = model.visual.output_dim if hasattr(model, 'visual') else 512\n",
        "\n",
        "    out = []\n",
        "    for sid in tqdm.tqdm(df['sample_id'].tolist(), desc=f'encode {out_path.name}'):\n",
        "        p = load_image_for_sid(sid)\n",
        "        if p is None:\n",
        "            out.append(np.zeros(emb_dim, dtype=np.float32))\n",
        "            continue\n",
        "        try:\n",
        "            im = Image.open(p).convert('RGB')\n",
        "            x = preprocess(im).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = model.encode_image(x) if use_openai_clip else model.encode_image(x)\n",
        "                emb = emb.cpu().numpy().reshape(-1)\n",
        "            out.append(emb)\n",
        "        except Exception:\n",
        "            out.append(np.zeros(emb_dim, dtype=np.float32))\n",
        "    out = np.vstack(out)\n",
        "    np.save(out_path, out)\n",
        "    return out\n",
        "\n",
        "IMG_EMB_TRAIN = Path(EMB_DIR)/'img_train.npy'\n",
        "IMG_EMB_TEST  = Path(EMB_DIR)/'img_test.npy'\n",
        "if IMG_EMB_TRAIN.exists() and IMG_EMB_TEST.exists():\n",
        "    img_emb_train = np.load(IMG_EMB_TRAIN)\n",
        "    img_emb_test  = np.load(IMG_EMB_TEST)\n",
        "else:\n",
        "    img_emb_train = encode_images(train, IMG_EMB_TRAIN)\n",
        "    img_emb_test  = encode_images(test, IMG_EMB_TEST)\n",
        "print('img_emb_train', img_emb_train.shape)\n"
      ],
      "id": "ETRVYW1g__pq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img_emb_train (75000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YNBrgEM__pr",
        "outputId": "87392319-3091-406f-a433-7b299df38e5a"
      },
      "source": [
        "# Text embeddings (SBERT)\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    raise RuntimeError('Please install sentence-transformers')\n",
        "sbert_model = 'all-MiniLM-L6-v2'\n",
        "TEXT_EMB_TRAIN = Path(EMB_DIR)/'text_train.npy'\n",
        "TEXT_EMB_TEST  = Path(EMB_DIR)/'text_test.npy'\n",
        "if TEXT_EMB_TRAIN.exists() and TEXT_EMB_TEST.exists():\n",
        "    text_emb_train = np.load(TEXT_EMB_TRAIN)\n",
        "    text_emb_test  = np.load(TEXT_EMB_TEST)\n",
        "else:\n",
        "    print('Loading SBERT model', sbert_model)\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    sbert = SentenceTransformer(sbert_model, device=device)\n",
        "    sbert._first_module().auto_model.half()\n",
        "    sbert.max_seq_length = 256\n",
        "\n",
        "    def chunked_encode(texts, chunks=8, bs=64):\n",
        "        out = []\n",
        "        n   = len(texts)\n",
        "        for i in range(chunks):\n",
        "            sub = texts[i*n//chunks:(i+1)*n//chunks]\n",
        "            emb = sbert.encode(sub, batch_size=bs, convert_to_numpy=True)\n",
        "            out.append(emb)\n",
        "        return l2_normalize_rows(np.vstack(out))\n",
        "\n",
        "    text_emb_train = chunked_encode(train['catalog_content'].tolist())\n",
        "    text_emb_test  = chunked_encode(test['catalog_content'].tolist())\n",
        "\n",
        "    tmp_train = TEXT_EMB_TRAIN.with_suffix('.tmp.npy')\n",
        "    tmp_test  = TEXT_EMB_TEST.with_suffix('.tmp.npy')\n",
        "    np.save(tmp_train, text_emb_train); tmp_train.replace(TEXT_EMB_TRAIN)\n",
        "    np.save(tmp_test,  text_emb_test);  tmp_test.replace(TEXT_EMB_TEST)\n",
        "print('text_emb_train', text_emb_train.shape)\n"
      ],
      "id": "_YNBrgEM__pr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_emb_train (75000, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "# Text embeddings (SBERT) with model fallback + L2 normalization\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "except Exception:\n",
        "    raise RuntimeError('Please install sentence-transformers')\n",
        "\n",
        "def l2_normalize_rows(a, eps=1e-8):\n",
        "    a = a.astype(np.float32, copy=False)\n",
        "    norms = np.linalg.norm(a, axis=1, keepdims=True)\n",
        "    return a / (norms + eps)\n",
        "\n",
        "# Prefer stronger checkpoint, with safe fallbacks\n",
        "candidates = ['all-mpnet-base-v2', 'all-MiniLM-L12-v2', 'all-MiniLM-L6-v2']\n",
        "TEXT_EMB_TRAIN = Path(EMB_DIR)/'text_train.npy'\n",
        "TEXT_EMB_TEST  = Path(EMB_DIR)/'text_test.npy'\n",
        "MODEL_TAG      = Path(EMB_DIR)/'text_model.txt'\n",
        "\n",
        "if TEXT_EMB_TRAIN.exists() and TEXT_EMB_TEST.exists():\n",
        "    text_emb_train = np.load(TEXT_EMB_TRAIN)\n",
        "    text_emb_test  = np.load(TEXT_EMB_TEST)\n",
        "else:\n",
        "    loaded = False\n",
        "    for sbert_model in candidates:\n",
        "        try:\n",
        "            print('Loading SBERT model', sbert_model)\n",
        "            sbert = SentenceTransformer(sbert_model)\n",
        "            text_emb_train = sbert.encode(\n",
        "                train['catalog_content'].tolist(),\n",
        "                batch_size=64, show_progress_bar=True, convert_to_numpy=True\n",
        "            )\n",
        "            text_emb_test  = sbert.encode(\n",
        "                test['catalog_content'].tolist(),\n",
        "                batch_size=64, show_progress_bar=True, convert_to_numpy=True\n",
        "            )\n",
        "            # L2 normalize\n",
        "            text_emb_train = l2_normalize_rows(text_emb_train)\n",
        "            text_emb_test  = l2_normalize_rows(text_emb_test)\n",
        "            # Cache\n",
        "            np.save(TEXT_EMB_TRAIN, text_emb_train)\n",
        "            np.save(TEXT_EMB_TEST, text_emb_test)\n",
        "            with open(MODEL_TAG, 'w') as f:\n",
        "                f.write(sbert_model)\n",
        "            loaded = True\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print('Failed on', sbert_model, '->', e)\n",
        "    if not loaded:\n",
        "        raise RuntimeError('Could not load any Sentence-Transformers model.')\n",
        "\n",
        "print('text_emb_train', text_emb_train.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfs0f-DEWMu9",
        "outputId": "9d77d3db-ce0c-4320-9a76-2f090e8c4edb"
      },
      "id": "xfs0f-DEWMu9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_emb_train (75000, 384)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v6Araqr__ps",
        "outputId": "a75cb7bd-f89c-4fc1-a43e-9c569396d3da"
      },
      "source": [
        "# Tabular features\n",
        "train['brand_guess'] = train['brand_guess'].fillna('').astype(str)\n",
        "brand_freq = train['brand_guess'].value_counts().to_dict()\n",
        "train['brand_freq'] = train['brand_guess'].map(lambda x: brand_freq.get(x,0)).astype(float)\n",
        "test['brand_freq']  = test['brand_guess'].map(lambda x: brand_freq.get(x,0)).fillna(0).astype(float)\n",
        "tab_cols = ['pack_count','size_val','norm_qty','total_qty','brand_freq']\n",
        "tab_train = train[tab_cols].fillna(0).values.astype(float)\n",
        "tab_test  = test[tab_cols].fillna(0).values.astype(float)\n",
        "\n",
        "# PCA dims (reduce SBERT & CLIP dims for speed)\n",
        "PCA_TEXT_DIM = 128\n",
        "PCA_IMG_DIM  = 128\n",
        "pca_text_path = Path(EMB_DIR)/'pca_text.joblib'\n",
        "pca_img_path  = Path(EMB_DIR)/'pca_img.joblib'\n",
        "if pca_text_path.exists():\n",
        "    pca_text = joblib.load(pca_text_path)\n",
        "    text_p_train = pca_text.transform(text_emb_train)\n",
        "    text_p_test  = pca_text.transform(text_emb_test)\n",
        "else:\n",
        "    pca_text = PCA(n_components=PCA_TEXT_DIM, random_state=SEED)\n",
        "    text_p_train = pca_text.fit_transform(text_emb_train)\n",
        "    text_p_test  = pca_text.transform(text_emb_test)\n",
        "    joblib.dump(pca_text, pca_text_path)\n",
        "if pca_img_path.exists():\n",
        "    pca_img = joblib.load(pca_img_path)\n",
        "    img_p_train = pca_img.transform(img_emb_train)\n",
        "    img_p_test  = pca_img.transform(img_emb_test)\n",
        "else:\n",
        "    pca_img = PCA(n_components=PCA_IMG_DIM, random_state=SEED)\n",
        "    img_p_train = pca_img.fit_transform(img_emb_train)\n",
        "    img_p_test  = pca_img.transform(img_emb_test)\n",
        "    joblib.dump(pca_img, pca_img_path)\n",
        "print('Reduced shapes:', text_p_train.shape, img_p_train.shape)\n",
        "\n",
        "X_train = np.hstack([text_p_train, img_p_train, tab_train])\n",
        "X_test  = np.hstack([text_p_test, img_p_test, tab_test])\n",
        "y_train = np.log1p(train['price'].values)\n",
        "print('Final feature shapes', X_train.shape, X_test.shape)\n"
      ],
      "id": "6v6Araqr__ps",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced shapes: (75000, 128) (75000, 128)\n",
            "Final feature shapes (75000, 261) (75000, 261)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "# === Tabular features (extended) ===\n",
        "# brand_freq kept; add features engineered earlier\n",
        "train['brand_guess'] = train['brand_guess'].fillna('').astype(str)\n",
        "brand_freq = train['brand_guess'].value_counts().to_dict()\n",
        "train['brand_freq'] = train['brand_guess'].map(lambda x: brand_freq.get(x,0)).astype(float)\n",
        "test['brand_freq']  = test['brand_guess'].map(lambda x: brand_freq.get(x,0)).fillna(0).astype(float)\n",
        "\n",
        "# Ensure these were created in the FE cell:\n",
        "# ['has_qty','log_total_qty','qty_per_pack','has_pack','brand_te'] if TE added\n",
        "extra_num = []\n",
        "for col in ['has_qty','log_total_qty','qty_per_pack','has_pack','brand_te']:\n",
        "    if col in train.columns:\n",
        "        extra_num.append(col)\n",
        "\n",
        "tab_cols = ['pack_count','size_val','norm_qty','total_qty','brand_freq'] + extra_num\n",
        "tab_train = train[tab_cols].fillna(0).values.astype(float)\n",
        "tab_test  = test[tab_cols].fillna(0).values.astype(float)\n",
        "\n",
        "# === PCA for SBERT & CLIP ===\n",
        "PCA_TEXT_DIM = 128\n",
        "PCA_IMG_DIM  = 128\n",
        "pca_text_path = Path(EMB_DIR)/'pca_text.joblib'\n",
        "pca_img_path  = Path(EMB_DIR)/'pca_img.joblib'\n",
        "\n",
        "if pca_text_path.exists():\n",
        "    pca_text = joblib.load(pca_text_path)\n",
        "    text_p_train = pca_text.transform(text_emb_train)\n",
        "    text_p_test  = pca_text.transform(text_emb_test)\n",
        "else:\n",
        "    pca_text = PCA(n_components=PCA_TEXT_DIM, random_state=SEED)\n",
        "    text_p_train = pca_text.fit_transform(text_emb_train)\n",
        "    text_p_test  = pca_text.transform(text_emb_test)\n",
        "    joblib.dump(pca_text, pca_text_path)\n",
        "\n",
        "if pca_img_path.exists():\n",
        "    pca_img = joblib.load(pca_img_path)\n",
        "    img_p_train = pca_img.transform(img_emb_train)\n",
        "    img_p_test  = pca_img.transform(img_emb_test)\n",
        "else:\n",
        "    pca_img = PCA(n_components=PCA_IMG_DIM, random_state=SEED)\n",
        "    img_p_train = pca_img.fit_transform(img_emb_train)\n",
        "    img_p_test  = pca_img.transform(img_emb_test)\n",
        "    joblib.dump(pca_img, pca_img_path)\n",
        "\n",
        "print('Reduced shapes:', text_p_train.shape, img_p_train.shape)\n",
        "\n",
        "# === TF-IDF -> SVD channel (expects tfv/svd already fit earlier; if not, fit here) ===\n",
        "if 'svd_train' in globals() and 'svd_test' in globals():\n",
        "    svd_train_use = svd_train\n",
        "    svd_test_use  = svd_test\n",
        "else:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.decomposition import TruncatedSVD\n",
        "    tfv = TfidfVectorizer(min_df=5, max_df=0.9, ngram_range=(1,2), max_features=50000)\n",
        "    tfv_train = tfv.fit_transform(train['catalog_content'])\n",
        "    tfv_test  = tfv.transform(test['catalog_content'])\n",
        "    svd = TruncatedSVD(n_components=128, random_state=SEED)\n",
        "    svd_train_use = svd.fit_transform(tfv_train)\n",
        "    svd_test_use  = svd.transform(tfv_test)\n",
        "\n",
        "# === Assemble design matrices ===\n",
        "X_train = np.hstack([text_p_train, img_p_train, svd_train_use, tab_train])\n",
        "X_test  = np.hstack([text_p_test,  img_p_test,  svd_test_use,  tab_test])\n",
        "\n",
        "# === Targets in log space with winsorization (ensure lo,hi computed earlier) ===\n",
        "if 'y_log_clipped' in globals():\n",
        "    y_train = y_log_clipped\n",
        "else:\n",
        "    y_true_price = train['price'].values\n",
        "    y_log = np.log1p(y_true_price)\n",
        "    lo, hi = np.percentile(y_log, [0.5, 99.5])\n",
        "    y_train = np.clip(y_log, lo, hi)\n",
        "\n",
        "print('Final feature shapes', X_train.shape, X_test.shape)\n",
        "\n",
        "# === Standardize X for linear/MLP models ===\n",
        "scaler = StandardScaler(with_mean=True, with_std=True)\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZvyakziOXS4X",
        "outputId": "0d3b0569-7c8a-48c7-9cd0-9eebd236ea20"
      },
      "id": "ZvyakziOXS4X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced shapes: (75000, 128) (75000, 128)\n",
            "Final feature shapes (75000, 393) (75000, 393)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FO9cKwtl__pt",
        "outputId": "33ac11bd-c59e-4dde-9a43-65e6202100a2"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "class SmallRegressor(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 512), nn.ReLU(), nn.Dropout(0.2),\n",
        "            nn.Linear(512, 128), nn.ReLU(), nn.Dropout(0.1),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def train_epoch(model, opt, loader, device):\n",
        "    model.train(); total_loss=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(xb).squeeze(1)\n",
        "        loss = torch.mean(torch.abs(out - yb))\n",
        "        loss.backward(); opt.step()\n",
        "        total_loss += loss.item()*xb.size(0)\n",
        "    return total_loss/len(loader.dataset)\n",
        "\n",
        "def valid_epoch(model, loader, device):\n",
        "    model.eval(); preds=[]; trues=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            out = model(xb).squeeze(1)\n",
        "            preds.append(out.cpu().numpy()); trues.append(yb.cpu().numpy())\n",
        "    return np.concatenate(preds), np.concatenate(trues)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device', device)\n",
        "scaler = StandardScaler()\n",
        "X_train_s = scaler.fit_transform(X_train)\n",
        "X_test_s  = scaler.transform(X_test)\n",
        "joblib.dump(scaler, Path(EMB_DIR)/'scaler.joblib')\n",
        "\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "oof_mlp = np.zeros(len(train)); test_mlp = np.zeros(len(test))\n",
        "oof_ridge = np.zeros(len(train)); test_ridge = np.zeros(len(test))\n",
        "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_s)):\n",
        "    print(f'Fold {fold+1}/{n_splits}')\n",
        "    X_tr, X_va = X_train_s[tr_idx], X_train_s[va_idx]\n",
        "    y_tr, y_va = y_train[tr_idx], y_train[va_idx]\n",
        "    # MLP\n",
        "    model = SmallRegressor(X_tr.shape[1]).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.5, patience=2, verbose=True)\n",
        "    tr_ds = TensorDataset(torch.tensor(X_tr,dtype=torch.float32), torch.tensor(y_tr,dtype=torch.float32))\n",
        "    va_ds = TensorDataset(torch.tensor(X_va,dtype=torch.float32), torch.tensor(y_va,dtype=torch.float32))\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=256, shuffle=True)\n",
        "    va_loader = DataLoader(va_ds, batch_size=256, shuffle=False)\n",
        "    best_val = 1e9; best_state=None\n",
        "    for epoch in range(1,7):\n",
        "        tr_loss = train_epoch(model, opt, tr_loader, device)\n",
        "        val_preds_log, val_trues_log = valid_epoch(model, va_loader, device)\n",
        "        val_mae = np.mean(np.abs(val_preds_log - val_trues_log))\n",
        "        print(f' Epoch {epoch}: tr_loss={tr_loss:.5f} val_mae_log={val_mae:.5f}')\n",
        "        if val_mae < best_val:\n",
        "            best_val = val_mae; best_state={k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
        "    model.load_state_dict(best_state)\n",
        "    # oof\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        oof_mlp[va_idx] = model(torch.tensor(X_va,dtype=torch.float32).to(device)).cpu().numpy().squeeze()\n",
        "        # test preds\n",
        "        preds = []\n",
        "        bs=512\n",
        "        for i in range(0, len(X_test_s), bs):\n",
        "            xb = torch.tensor(X_test_s[i:i+bs], dtype=torch.float32).to(device)\n",
        "            preds.append(model(xb).cpu().numpy())\n",
        "        test_mlp += np.concatenate(preds).squeeze() / n_splits\n",
        "    # Ridge\n",
        "    ridge = Ridge(alpha=1.0)\n",
        "    ridge.fit(X_tr, y_tr)\n",
        "    oof_ridge[va_idx] = ridge.predict(X_va)\n",
        "    test_ridge += ridge.predict(X_test_s) / n_splits\n",
        "print('Converting back to price space and computing SMAPE...')\n",
        "def smape(y_true, y_pred):\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    denom_safe = np.where(denom==0, 1.0, denom)\n",
        "    return np.mean(np.abs(y_true-y_pred)/denom_safe)*100\n",
        "oof_mlp_price = np.expm1(oof_mlp)\n",
        "oof_ridge_price = np.expm1(oof_ridge)\n",
        "print('MLP OOF SMAPE:', smape(train['price'].values, oof_mlp_price))\n",
        "print('Ridge OOF SMAPE:', smape(train['price'].values, oof_ridge_price))\n"
      ],
      "id": "FO9cKwtl__pt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n",
            "Fold 1/5\n",
            " Epoch 1: tr_loss=0.70308 val_mae_log=0.61639\n",
            " Epoch 2: tr_loss=0.58797 val_mae_log=0.59957\n",
            " Epoch 3: tr_loss=0.56427 val_mae_log=0.58880\n",
            " Epoch 4: tr_loss=0.54974 val_mae_log=0.58203\n",
            " Epoch 5: tr_loss=0.53619 val_mae_log=0.58850\n",
            " Epoch 6: tr_loss=0.52788 val_mae_log=0.57164\n",
            "Fold 2/5\n",
            " Epoch 1: tr_loss=0.68924 val_mae_log=0.59757\n",
            " Epoch 2: tr_loss=0.58852 val_mae_log=0.58872\n",
            " Epoch 3: tr_loss=0.56522 val_mae_log=0.56694\n",
            " Epoch 4: tr_loss=0.54802 val_mae_log=0.55710\n",
            " Epoch 5: tr_loss=0.53916 val_mae_log=0.56297\n",
            " Epoch 6: tr_loss=0.52667 val_mae_log=0.55286\n",
            "Fold 3/5\n",
            " Epoch 1: tr_loss=0.69368 val_mae_log=0.60344\n",
            " Epoch 2: tr_loss=0.58915 val_mae_log=0.58040\n",
            " Epoch 3: tr_loss=0.56573 val_mae_log=0.56735\n",
            " Epoch 4: tr_loss=0.54935 val_mae_log=0.55691\n",
            " Epoch 5: tr_loss=0.53703 val_mae_log=0.55902\n",
            " Epoch 6: tr_loss=0.52984 val_mae_log=0.54927\n",
            "Fold 4/5\n",
            " Epoch 1: tr_loss=0.70127 val_mae_log=0.58104\n",
            " Epoch 2: tr_loss=0.59334 val_mae_log=0.57145\n",
            " Epoch 3: tr_loss=0.56807 val_mae_log=0.55718\n",
            " Epoch 4: tr_loss=0.55440 val_mae_log=0.55168\n",
            " Epoch 5: tr_loss=0.54173 val_mae_log=0.54768\n",
            " Epoch 6: tr_loss=0.53210 val_mae_log=0.54240\n",
            "Fold 5/5\n",
            " Epoch 1: tr_loss=0.68704 val_mae_log=0.60458\n",
            " Epoch 2: tr_loss=0.58873 val_mae_log=0.58692\n",
            " Epoch 3: tr_loss=0.56461 val_mae_log=0.57454\n",
            " Epoch 4: tr_loss=0.54735 val_mae_log=0.57883\n",
            " Epoch 5: tr_loss=0.53579 val_mae_log=0.56666\n",
            " Epoch 6: tr_loss=0.52538 val_mae_log=0.57067\n",
            "Converting back to price space and computing SMAPE...\n",
            "MLP OOF SMAPE: 55.46848872186314\n",
            "Ridge OOF SMAPE: 60.09062345656256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Method V1"
      ],
      "metadata": {
        "id": "1q4OL7a2FqHD"
      },
      "id": "1q4OL7a2FqHD"
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# --- Small MLP with BatchNorm and dropout ---\n",
        "# AFTER\n",
        "class BalancedRegressor(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 768),\n",
        "            nn.BatchNorm1d(768),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.15),  # Lower dropout\n",
        "            nn.Linear(768, 384),\n",
        "            nn.BatchNorm1d(384),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(384, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.05),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "    def forward(self, x): return self.net(x)\n",
        "\n",
        "\n",
        "# --- Train/valid with Huber loss in log-space ---\n",
        "def train_epoch(model, opt, loader, device, criterion):\n",
        "    model.train(); total_loss=0.0\n",
        "    for xb, yb in loader:\n",
        "        xb = xb.to(device); yb = yb.to(device)\n",
        "        opt.zero_grad()\n",
        "        out = model(xb).squeeze(1)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward(); opt.step()\n",
        "        total_loss += loss.item()*xb.size(0)\n",
        "    return total_loss/len(loader.dataset)\n",
        "\n",
        "def valid_epoch(model, loader, device):\n",
        "    model.eval(); preds=[]; trues=[]\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device); yb = yb.to(device)\n",
        "            out = model(xb).squeeze(1)\n",
        "            preds.append(out.cpu().numpy()); trues.append(yb.cpu().numpy())\n",
        "    return np.concatenate(preds), np.concatenate(trues)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device', device)\n",
        "\n",
        "# X_train_s, X_test_s, y_train (winsorized log) should be defined earlier\n",
        "joblib.dump(scaler, Path(EMB_DIR)/'scaler.joblib')\n",
        "\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
        "\n",
        "oof_mlp_log   = np.zeros(len(train)); pred_mlp_log   = np.zeros(len(test))\n",
        "oof_ridge_log = np.zeros(len(train)); pred_ridge_log = np.zeros(len(test))\n",
        "# Initialize pred_lgb_log and oof_ensemble_log before the loop\n",
        "pred_lgb_log = np.zeros(len(test))\n",
        "oof_ensemble_log = np.zeros(len(train))\n",
        "\n",
        "for fold, (tr_idx, va_idx) in enumerate(kf.split(X_train_s)):\n",
        "    print(f'Fold {fold+1}/{n_splits}')\n",
        "    X_tr, X_va = X_train_s[tr_idx], X_train_s[va_idx]\n",
        "    y_tr, y_va = y_train[tr_idx], y_train[va_idx]  # log space\n",
        "\n",
        "    # --- MLP ---\n",
        "    model = BalancedRegressor(X_tr.shape[1]).to(device)\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)  # Lower LR, less decay\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min', factor=0.7, patience=3)\n",
        "\n",
        "    criterion = nn.SmoothL1Loss(beta=0.1)  # Huber in log-space\n",
        "\n",
        "    tr_ds = TensorDataset(torch.tensor(X_tr,dtype=torch.float32), torch.tensor(y_tr,dtype=torch.float32))\n",
        "    va_ds = TensorDataset(torch.tensor(X_va,dtype=torch.float32), torch.tensor(y_va,dtype=torch.float32))\n",
        "    tr_loader = DataLoader(tr_ds, batch_size=256, shuffle=True, drop_last=False)\n",
        "    va_loader = DataLoader(va_ds, batch_size=256, shuffle=False, drop_last=False)\n",
        "\n",
        "    best_val = 1e9; best_state=None; wait=0; patience=4\n",
        "    for epoch in range(1, 50):  # up to 30 epochs\n",
        "        tr_loss = train_epoch(model, opt, tr_loader, device, criterion)\n",
        "        val_preds_log, val_trues_log = valid_epoch(model, va_loader, device)\n",
        "        val_mae = np.mean(np.abs(val_preds_log - val_trues_log))\n",
        "        scheduler.step(val_mae)\n",
        "        print(f' Epoch {epoch}: tr_loss={tr_loss:.5f} val_mae_log={val_mae:.5f}')\n",
        "        if val_mae < best_val - 1e-4:\n",
        "            best_val = val_mae; best_state={k:v.cpu().clone() for k,v in model.state_dict().items()}\n",
        "            wait=0\n",
        "        else:\n",
        "            wait += 1\n",
        "            # Decay learning rate for MLP\n",
        "            for g in opt.param_groups: g['lr'] *= 0.5\n",
        "            if wait >= patience:\n",
        "                break\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # OOF and test predictions (log space)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        oof_mlp_log[va_idx] = model(torch.tensor(X_va,dtype=torch.float32).to(device)).cpu().numpy().squeeze()\n",
        "        preds = []\n",
        "        bs=512\n",
        "        for i in range(0, len(X_test_s), bs):\n",
        "            xb = torch.tensor(X_test_s[i:i+bs], dtype=torch.float32).to(device)\n",
        "            preds.append(model(xb).cpu().numpy())\n",
        "        pred_mlp_log += np.concatenate(preds).squeeze() / n_splits\n",
        "\n",
        "    # --- Ridge (stabilized in log-space with y-scaler) ---\n",
        "    # Use the same scaler as for MLP\n",
        "    # x_scaler = StandardScaler(with_mean=False, with_std=False)  # already standardized; keep identity\n",
        "    X_tr_r, X_va_r = X_train_s[tr_idx], X_train_s[va_idx] # Use scaled data\n",
        "\n",
        "    y_scaler = StandardScaler().fit(y_train[tr_idx].reshape(-1,1))\n",
        "    ridge = Ridge(alpha=1.0, random_state=SEED)\n",
        "    ridge.fit(X_tr_r, y_scaler.transform(y_train[tr_idx].reshape(-1,1)).ravel())\n",
        "\n",
        "    pred_va_std = ridge.predict(X_va_r)\n",
        "    oof_ridge_log[va_idx] = y_scaler.inverse_transform(pred_va_std.reshape(-1,1)).ravel()\n",
        "\n",
        "    # For test, refit on full train each fold or reuse final; here we average across folds\n",
        "    pred_test_std = ridge.predict(X_test_s)\n",
        "    pred_ridge_log += y_scaler.inverse_transform(pred_test_std.reshape(-1,1)).ravel() / n_splits\n",
        "\n",
        "    # --- LightGBM stacking (train on log-space for better blending) ---\n",
        "    import lightgbm as lgb\n",
        "    lgb_params = {\n",
        "        'objective': 'regression', 'metric': 'l1', 'learning_rate': 0.03,  # Slower LR\n",
        "        'num_leaves': 63, 'max_depth': 7, 'feature_fraction': 0.85,  # More leaves/depth\n",
        "        'bagging_fraction': 0.85, 'min_child_samples': 50, 'verbosity': -1,\n",
        "        'seed': SEED # Add seed for reproducibility\n",
        "    }\n",
        "    y_tr_log = y_train[tr_idx] # y_train is already winsorized log\n",
        "    y_va_log = y_train[va_idx]\n",
        "\n",
        "    lgb_train = lgb.Dataset(X_tr, y_tr_log)\n",
        "    lgb_valid = lgb.Dataset(X_va, y_va_log, reference=lgb_train)\n",
        "    lgb_model = lgb.train(\n",
        "        lgb_params, lgb_train, num_boost_round=500,  # More rounds\n",
        "        valid_sets=[lgb_valid],\n",
        "        callbacks=[lgb.early_stopping(30), lgb.log_evaluation(0)]\n",
        "    )\n",
        "\n",
        "    # OOF prediction for LGBM\n",
        "    pred_va_lgb_log = lgb_model.predict(X_va)\n",
        "\n",
        "    # Accumulate LGB test preds separately\n",
        "    pred_lgb_log += lgb_model.predict(X_test_s) / n_splits  # Global pred_lgb_log init outside loop\n",
        "\n",
        "    # === OOF Blending in log-space ===\n",
        "    # Blend OOF in log-space (weighted for SMAPE)\n",
        "    # Use pred_va_lgb_log for OOF LGBM\n",
        "    oof_ensemble_log[va_idx] = (0.4 * oof_mlp_log[va_idx] +\n",
        "                                0.35 * oof_ridge_log[va_idx] +\n",
        "                                0.25 * pred_va_lgb_log)\n",
        "\n",
        "# === Ensemble test predictions in log-space ===\n",
        "# Use the accumulated test predictions\n",
        "blend_test_log = (0.4 * pred_mlp_log +\n",
        "                  0.35 * pred_ridge_log +\n",
        "                  0.25 * pred_lgb_log)\n",
        "\n",
        "\n",
        "print('Converting back to price space and computing SMAPE...')\n",
        "\n",
        "# --- Safe inverse + SMAPE ---\n",
        "def safe_expm1_clip(y_log, lo, hi): # Use lo, hi from winsorization\n",
        "    y_log = np.clip(y_log, lo, hi).astype(np.float64)\n",
        "    y = np.expm1(y_log)\n",
        "    return np.clip(y, 0.01, None)\n",
        "\n",
        "def smape_safe(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.float64); y_pred = y_pred.astype(np.float64)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    denom = np.maximum(denom, 1.0)\n",
        "    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n",
        "\n",
        "# If you computed lo,hi earlier from winsorization, reuse them; else defaults are fine\n",
        "# Assuming lo and hi are already defined from the data wrangling stage\n",
        "# If not, uncomment and run the following lines:\n",
        "if 'lo' not in globals() or 'hi' not in globals():\n",
        "    y_true_price = train['price'].values\n",
        "    y_log = np.log1p(y_true_price)\n",
        "    lo, hi = np.percentile(y_log, [0.5, 99.5])\n",
        "\n",
        "\n",
        "# Evaluate ensemble OOF SMAPE\n",
        "oof_ensemble_price = safe_expm1_clip(oof_ensemble_log, lo, hi)\n",
        "ensemble_smape = smape_safe(train['price'].values, oof_ensemble_price)\n",
        "print('Ensemble OOF SMAPE:', ensemble_smape)\n",
        "\n",
        "# No need to print individual MLP and Ridge SMAPE here, as the focus is on the ensemble\n",
        "# print('MLP OOF SMAPE:', smape_safe(train['price'].values, safe_expm1_clip(oof_mlp_log, lo, hi)))\n",
        "# print('Ridge OOF SMAPE:', smape_safe(train['price'].values, safe_expm1_clip(oof_ridge_log, lo, hi)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qblBeqQFue0",
        "outputId": "d02988a2-f91f-41f1-eaff-012c9baf8ca0"
      },
      "id": "1qblBeqQFue0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n",
            "Fold 1/5\n",
            " Epoch 1: tr_loss=0.96619 val_mae_log=0.59676\n",
            " Epoch 2: tr_loss=0.52409 val_mae_log=0.56979\n",
            " Epoch 3: tr_loss=0.48329 val_mae_log=0.56791\n",
            " Epoch 4: tr_loss=0.45854 val_mae_log=0.55266\n",
            " Epoch 5: tr_loss=0.43919 val_mae_log=0.54349\n",
            " Epoch 6: tr_loss=0.42414 val_mae_log=0.54236\n",
            " Epoch 7: tr_loss=0.40917 val_mae_log=0.54109\n",
            " Epoch 8: tr_loss=0.39830 val_mae_log=0.54005\n",
            " Epoch 9: tr_loss=0.38537 val_mae_log=0.53452\n",
            " Epoch 10: tr_loss=0.37705 val_mae_log=0.53402\n",
            " Epoch 11: tr_loss=0.36973 val_mae_log=0.53576\n",
            " Epoch 12: tr_loss=0.34179 val_mae_log=0.52401\n",
            " Epoch 13: tr_loss=0.33173 val_mae_log=0.52603\n",
            " Epoch 14: tr_loss=0.31485 val_mae_log=0.52027\n",
            " Epoch 15: tr_loss=0.30925 val_mae_log=0.52272\n",
            " Epoch 16: tr_loss=0.29981 val_mae_log=0.52172\n",
            " Epoch 17: tr_loss=0.29565 val_mae_log=0.52028\n",
            " Epoch 18: tr_loss=0.29156 val_mae_log=0.51973\n",
            " Epoch 19: tr_loss=0.29123 val_mae_log=0.51981\n",
            " Epoch 20: tr_loss=0.28931 val_mae_log=0.51927\n",
            " Epoch 21: tr_loss=0.28885 val_mae_log=0.51929\n",
            " Epoch 22: tr_loss=0.28770 val_mae_log=0.51980\n",
            " Epoch 23: tr_loss=0.28755 val_mae_log=0.51926\n",
            " Epoch 24: tr_loss=0.28739 val_mae_log=0.51882\n",
            " Epoch 25: tr_loss=0.28812 val_mae_log=0.51906\n",
            " Epoch 26: tr_loss=0.28713 val_mae_log=0.51938\n",
            " Epoch 27: tr_loss=0.28761 val_mae_log=0.51937\n",
            " Epoch 28: tr_loss=0.28809 val_mae_log=0.51961\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's l1: 0.551982\n",
            "Fold 2/5\n",
            " Epoch 1: tr_loss=0.97190 val_mae_log=0.62873\n",
            " Epoch 2: tr_loss=0.52700 val_mae_log=0.56321\n",
            " Epoch 3: tr_loss=0.48752 val_mae_log=0.54768\n",
            " Epoch 4: tr_loss=0.46143 val_mae_log=0.54928\n",
            " Epoch 5: tr_loss=0.42349 val_mae_log=0.52906\n",
            " Epoch 6: tr_loss=0.40882 val_mae_log=0.52572\n",
            " Epoch 7: tr_loss=0.39764 val_mae_log=0.52277\n",
            " Epoch 8: tr_loss=0.38817 val_mae_log=0.52546\n",
            " Epoch 9: tr_loss=0.36852 val_mae_log=0.51574\n",
            " Epoch 10: tr_loss=0.36074 val_mae_log=0.51599\n",
            " Epoch 11: tr_loss=0.34940 val_mae_log=0.51439\n",
            " Epoch 12: tr_loss=0.34440 val_mae_log=0.51477\n",
            " Epoch 13: tr_loss=0.33890 val_mae_log=0.51373\n",
            " Epoch 14: tr_loss=0.33753 val_mae_log=0.51256\n",
            " Epoch 15: tr_loss=0.33444 val_mae_log=0.51340\n",
            " Epoch 16: tr_loss=0.33249 val_mae_log=0.51227\n",
            " Epoch 17: tr_loss=0.33222 val_mae_log=0.51236\n",
            " Epoch 18: tr_loss=0.32938 val_mae_log=0.51204\n",
            " Epoch 19: tr_loss=0.32796 val_mae_log=0.51223\n",
            " Epoch 20: tr_loss=0.32973 val_mae_log=0.51256\n",
            " Epoch 21: tr_loss=0.32872 val_mae_log=0.51254\n",
            " Epoch 22: tr_loss=0.32701 val_mae_log=0.51236\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's l1: 0.543544\n",
            "Fold 3/5\n",
            " Epoch 1: tr_loss=0.91459 val_mae_log=0.60043\n",
            " Epoch 2: tr_loss=0.52390 val_mae_log=0.56755\n",
            " Epoch 3: tr_loss=0.48589 val_mae_log=0.55044\n",
            " Epoch 4: tr_loss=0.45888 val_mae_log=0.54521\n",
            " Epoch 5: tr_loss=0.43950 val_mae_log=0.53710\n",
            " Epoch 6: tr_loss=0.42164 val_mae_log=0.53229\n",
            " Epoch 7: tr_loss=0.40927 val_mae_log=0.53850\n",
            " Epoch 8: tr_loss=0.37969 val_mae_log=0.52263\n",
            " Epoch 9: tr_loss=0.36783 val_mae_log=0.51957\n",
            " Epoch 10: tr_loss=0.35948 val_mae_log=0.51990\n",
            " Epoch 11: tr_loss=0.34073 val_mae_log=0.51513\n",
            " Epoch 12: tr_loss=0.33490 val_mae_log=0.51642\n",
            " Epoch 13: tr_loss=0.32605 val_mae_log=0.51424\n",
            " Epoch 14: tr_loss=0.32225 val_mae_log=0.51293\n",
            " Epoch 15: tr_loss=0.31657 val_mae_log=0.51277\n",
            " Epoch 16: tr_loss=0.31547 val_mae_log=0.51421\n",
            " Epoch 17: tr_loss=0.31113 val_mae_log=0.51393\n",
            " Epoch 18: tr_loss=0.30801 val_mae_log=0.51357\n",
            " Epoch 19: tr_loss=0.30573 val_mae_log=0.51221\n",
            " Epoch 20: tr_loss=0.30598 val_mae_log=0.51246\n",
            " Epoch 21: tr_loss=0.30547 val_mae_log=0.51253\n",
            " Epoch 22: tr_loss=0.30430 val_mae_log=0.51150\n",
            " Epoch 23: tr_loss=0.30630 val_mae_log=0.51242\n",
            " Epoch 24: tr_loss=0.30475 val_mae_log=0.51231\n",
            " Epoch 25: tr_loss=0.30505 val_mae_log=0.51159\n",
            " Epoch 26: tr_loss=0.30419 val_mae_log=0.51252\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's l1: 0.542684\n",
            "Fold 4/5\n",
            " Epoch 1: tr_loss=0.99109 val_mae_log=0.58122\n",
            " Epoch 2: tr_loss=0.52909 val_mae_log=0.54857\n",
            " Epoch 3: tr_loss=0.48954 val_mae_log=0.54791\n",
            " Epoch 4: tr_loss=0.46261 val_mae_log=0.54002\n",
            " Epoch 5: tr_loss=0.44472 val_mae_log=0.52839\n",
            " Epoch 6: tr_loss=0.42668 val_mae_log=0.52437\n",
            " Epoch 7: tr_loss=0.41471 val_mae_log=0.52296\n",
            " Epoch 8: tr_loss=0.40165 val_mae_log=0.52539\n",
            " Epoch 9: tr_loss=0.37427 val_mae_log=0.51283\n",
            " Epoch 10: tr_loss=0.35964 val_mae_log=0.51008\n",
            " Epoch 11: tr_loss=0.35169 val_mae_log=0.51189\n",
            " Epoch 12: tr_loss=0.33501 val_mae_log=0.50738\n",
            " Epoch 13: tr_loss=0.32823 val_mae_log=0.50746\n",
            " Epoch 14: tr_loss=0.31912 val_mae_log=0.50454\n",
            " Epoch 15: tr_loss=0.31488 val_mae_log=0.50592\n",
            " Epoch 16: tr_loss=0.31075 val_mae_log=0.50441\n",
            " Epoch 17: tr_loss=0.30859 val_mae_log=0.50498\n",
            " Epoch 18: tr_loss=0.30642 val_mae_log=0.50444\n",
            " Epoch 19: tr_loss=0.30312 val_mae_log=0.50314\n",
            " Epoch 20: tr_loss=0.30411 val_mae_log=0.50308\n",
            " Epoch 21: tr_loss=0.30373 val_mae_log=0.50329\n",
            " Epoch 22: tr_loss=0.30214 val_mae_log=0.50361\n",
            " Epoch 23: tr_loss=0.30274 val_mae_log=0.50526\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's l1: 0.531717\n",
            "Fold 5/5\n",
            " Epoch 1: tr_loss=0.90062 val_mae_log=0.59846\n",
            " Epoch 2: tr_loss=0.52246 val_mae_log=0.56920\n",
            " Epoch 3: tr_loss=0.48360 val_mae_log=0.55946\n",
            " Epoch 4: tr_loss=0.45997 val_mae_log=0.55207\n",
            " Epoch 5: tr_loss=0.43998 val_mae_log=0.54461\n",
            " Epoch 6: tr_loss=0.42552 val_mae_log=0.54145\n",
            " Epoch 7: tr_loss=0.41007 val_mae_log=0.53862\n",
            " Epoch 8: tr_loss=0.39880 val_mae_log=0.53547\n",
            " Epoch 9: tr_loss=0.38975 val_mae_log=0.53357\n",
            " Epoch 10: tr_loss=0.37979 val_mae_log=0.53341\n",
            " Epoch 11: tr_loss=0.37067 val_mae_log=0.53104\n",
            " Epoch 12: tr_loss=0.36073 val_mae_log=0.53372\n",
            " Epoch 13: tr_loss=0.33423 val_mae_log=0.52430\n",
            " Epoch 14: tr_loss=0.32569 val_mae_log=0.52260\n",
            " Epoch 15: tr_loss=0.31843 val_mae_log=0.52388\n",
            " Epoch 16: tr_loss=0.30330 val_mae_log=0.51971\n",
            " Epoch 17: tr_loss=0.29867 val_mae_log=0.51852\n",
            " Epoch 18: tr_loss=0.29463 val_mae_log=0.51972\n",
            " Epoch 19: tr_loss=0.28629 val_mae_log=0.51844\n",
            " Epoch 20: tr_loss=0.28022 val_mae_log=0.51719\n",
            " Epoch 21: tr_loss=0.27946 val_mae_log=0.51670\n",
            " Epoch 22: tr_loss=0.27866 val_mae_log=0.51720\n",
            " Epoch 23: tr_loss=0.27468 val_mae_log=0.51773\n",
            " Epoch 24: tr_loss=0.27513 val_mae_log=0.51756\n",
            " Epoch 25: tr_loss=0.27486 val_mae_log=0.51683\n",
            "Training until validation scores don't improve for 30 rounds\n",
            "Did not meet early stopping. Best iteration is:\n",
            "[500]\tvalid_0's l1: 0.54807\n",
            "Converting back to price space and computing SMAPE...\n",
            "Ensemble OOF SMAPE: 52.902522546747775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Q1lnS5__pu",
        "outputId": "d769a5f6-2dc4-4d1d-80cf-81d70b5e289a"
      },
      "source": [
        "# # Ensemble predictions and save submission\n",
        "# w_mlp = 0.7\n",
        "# w_ridge = 0.3\n",
        "# test_pred_price = w_mlp * np.expm1(test_mlp) + w_ridge * np.expm1(test_ridge)\n",
        "# low_cap, high_cap = np.percentile(train['price'].values, [0.5, 99.5])\n",
        "# test_pred_price = np.clip(test_pred_price, 0.01, high_cap*3.0)\n",
        "# sub = pd.DataFrame({'sample_id': test['sample_id'].values, 'price': test_pred_price})\n",
        "# out_file = Path(SUB_DIR)/'test_out.csv'\n",
        "# sub.to_csv(out_file, index=False)\n",
        "# print('Saved submission to', out_file)\n",
        "# # Save OOF for analysis\n",
        "# pd.DataFrame({'sample_id':train['sample_id'],'price_true':train['price'],'oof_mlp_log':oof_mlp,'oof_ridge_log':oof_ridge}).to_csv(Path(SUB_DIR)/'oof.csv', index=False)\n"
      ],
      "id": "f2Q1lnS5__pu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3256694622.py:4: RuntimeWarning: overflow encountered in expm1\n",
            "  test_pred_price = w_mlp * np.expm1(test_mlp) + w_ridge * np.expm1(test_ridge)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved submission to /content/drive/MyDrive/amazon ml challenge/68e8d1d70b66d_student_resource/student_resource/submissions/test_out.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55835435",
        "outputId": "8f42d3c5-346e-4716-e969-b10936fb42be"
      },
      "source": [
        "# === OOF-weighted blend and submission ===\n",
        "\n",
        "# Ensure these exist from the training cell:\n",
        "# oof_mlp_log, pred_mlp_log, oof_ridge_log, pred_ridge_log, pred_lgb_log\n",
        "# Also lo, hi from winsorization; if not present, define defaults:\n",
        "if 'lo' not in globals() or 'hi' not in globals():\n",
        "    y_log_temp = np.log1p(train['price'].values)\n",
        "    lo, hi = np.percentile(y_log_temp, [0.5, 99.5])\n",
        "\n",
        "def safe_expm1_clip(y_log, lo=-5.0, hi=12.0):\n",
        "    y_log = np.clip(y_log, lo, hi).astype(np.float64)\n",
        "    y = np.expm1(y_log)\n",
        "    return np.clip(y, 0.01, None)\n",
        "\n",
        "def smape_safe(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.float64); y_pred = y_pred.astype(np.float64)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    denom = np.maximum(denom, 1.0)\n",
        "    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n",
        "\n",
        "# Grid search non-negative weights that sum to 1 (3-model case)\n",
        "# weights should sum to 1: w_mlp + w_ridge + w_lgb = 1\n",
        "# We can iterate over two weights and derive the third\n",
        "best_err = 1e9; best_w = (1/3, 1/3, 1/3) # default\n",
        "y_true_price = train['price'].values\n",
        "\n",
        "# Define a finer grid for potentially better optimization\n",
        "grid_size = 11 # Increase for finer search, e.g., 21 for 0.05 steps\n",
        "grid = np.linspace(0, 1, grid_size)\n",
        "\n",
        "for w_mlp in grid:\n",
        "    for w_ridge in grid:\n",
        "        w_lgb = 1.0 - w_mlp - w_ridge\n",
        "        if w_lgb >= 0: # Ensure weights are non-negative\n",
        "            blend_oof_log = w_mlp * oof_mlp_log + w_ridge * oof_ridge_log + w_lgb * oof_ensemble_log # Note: oof_ensemble_log already contains blended MLP+Ridge+LGB, but this is for re-optimizing overall blend\n",
        "            blend_oof_price = safe_expm1_clip(blend_oof_log, lo, hi)\n",
        "            err = smape_safe(y_true_price, blend_oof_price)\n",
        "            if err < best_err:\n",
        "                best_err = err; best_w = (w_mlp, w_ridge, w_lgb)\n",
        "\n",
        "print(\"Best OOF weights (MLP, Ridge, LGB):\", best_w, \"OOF SMAPE:\", best_err)\n",
        "\n",
        "# Apply weights to test predictions (log space) and inverse safely\n",
        "w_mlp, w_ridge, w_lgb = best_w\n",
        "# Note: pred_lgb_log needs to be calculated from the individual LGB models if not already accumulated correctly\n",
        "# Assuming pred_lgb_log was accumulated in the training loop along with pred_mlp_log and pred_ridge_log\n",
        "blend_test_log = w_mlp * pred_mlp_log + w_ridge * pred_ridge_log + w_lgb * pred_lgb_log\n",
        "\n",
        "test_pred_price = safe_expm1_clip(blend_test_log, lo, hi)\n",
        "\n",
        "# Clip to reasonable bounds derived from train\n",
        "# Use slightly wider bounds for test predictions\n",
        "low_cap, high_cap = np.percentile(y_true_price, [0.5, 99.5])\n",
        "test_pred_price = np.clip(test_pred_price, 0.01, high_cap * 3.0)\n",
        "\n",
        "\n",
        "# Save submission\n",
        "sub = pd.DataFrame({'sample_id': test['sample_id'].values, 'price': test_pred_price})\n",
        "out_file = Path(SUB_DIR) / 'test_out.csv'\n",
        "sub.to_csv(out_file, index=False)\n",
        "print('Saved submission to', out_file)\n",
        "\n",
        "# Save OOF diagnostics (log and price)\n",
        "oof_mlp_price   = safe_expm1_clip(oof_mlp_log,   lo, hi)\n",
        "oof_ridge_price = safe_expm1_clip(oof_ridge_log, lo, hi)\n",
        "oof_lgb_price   = safe_expm1_clip(lgb_model.predict(X_train_s), lo, hi) # Using final LGB model for simplicity here\n",
        "\n",
        "oof_df = pd.DataFrame({\n",
        "    'sample_id': train['sample_id'],\n",
        "    'price_true': y_true_price,\n",
        "    'oof_mlp_log': oof_mlp_log,\n",
        "    'oof_ridge_log': oof_ridge_log,\n",
        "    'oof_lgb_log': lgb_model.predict(X_train_s),\n",
        "    'oof_blend_log': blend_oof_log,\n",
        "    'oof_mlp_price': oof_mlp_price,\n",
        "    'oof_ridge_price': oof_ridge_price,\n",
        "    'oof_lgb_price': oof_lgb_price,\n",
        "    'oof_blend_price': safe_expm1_clip(blend_oof_log, lo, hi) # Recalculate blend_oof_price using the best weights\n",
        "})\n",
        "oof_df.to_csv(Path(SUB_DIR)/'oof.csv', index=False)"
      ],
      "id": "55835435",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF weights (MLP, Ridge, LGB): (np.float64(0.7000000000000001), np.float64(0.0), np.float64(0.29999999999999993)) OOF SMAPE: 50.95796340947655\n",
            "Saved submission to /content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/submissions/test_out.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##\n",
        "# === OOF-weighted blend and submission ===\n",
        "\n",
        "# Ensure these exist from the training cell:\n",
        "# oof_mlp_log, pred_mlp_log, oof_ridge_log, pred_ridge_log\n",
        "# Also lo, hi from winsorization; if not present, define defaults:\n",
        "if 'lo' not in globals() or 'hi' not in globals():\n",
        "    y_log_temp = np.log1p(train['price'].values)\n",
        "    lo, hi = np.percentile(y_log_temp, [0.5, 99.5])\n",
        "\n",
        "def safe_expm1_clip(y_log, lo=-5.0, hi=12.0):\n",
        "    y_log = np.clip(y_log, lo, hi).astype(np.float64)\n",
        "    y = np.expm1(y_log)\n",
        "    return np.clip(y, 0.01, None)\n",
        "\n",
        "def smape_safe(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.float64); y_pred = y_pred.astype(np.float64)\n",
        "    denom = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
        "    denom = np.maximum(denom, 1.0)\n",
        "    return np.mean(np.abs(y_true - y_pred) / denom) * 100.0\n",
        "\n",
        "# Grid search non-negative weights that sum to 1 (2-model case)\n",
        "grid = np.linspace(0, 1, 21)  # 0.0 to 1.0 step 0.05\n",
        "best_err = 1e9; best_w = (0.7, 0.3)  # default\n",
        "y_true_price = train['price'].values\n",
        "\n",
        "for w in grid:\n",
        "    w_mlp, w_ridge = w, 1.0 - w\n",
        "    blend_oof_log = w_mlp * oof_mlp_log + w_ridge * oof_ridge_log\n",
        "    blend_oof_price = safe_expm1_clip(blend_oof_log, lo, hi)\n",
        "    err = smape_safe(y_true_price, blend_oof_price)\n",
        "    if err < best_err:\n",
        "        best_err = err; best_w = (w_mlp, w_ridge)\n",
        "\n",
        "print(\"Best OOF weights (MLP, Ridge):\", best_w, \"OOF SMAPE:\", best_err)\n",
        "\n",
        "# Apply weights to test predictions (log space) and inverse safely\n",
        "w_mlp, w_ridge = best_w\n",
        "blend_test_log = w_mlp * pred_mlp_log + w_ridge * pred_ridge_log\n",
        "test_pred_price = safe_expm1_clip(blend_test_log, lo, hi)\n",
        "\n",
        "# Clip to reasonable bounds derived from train\n",
        "low_cap, high_cap = np.percentile(y_true_price, [0.5, 99.5])\n",
        "test_pred_price = np.clip(test_pred_price, 0.01, high_cap * 3.0)\n",
        "\n",
        "# Save submission\n",
        "sub = pd.DataFrame({'sample_id': test['sample_id'].values, 'price': test_pred_price})\n",
        "out_file = Path(SUB_DIR) / 'test_out.csv'\n",
        "sub.to_csv(out_file, index=False)\n",
        "print('Saved submission to', out_file)\n",
        "\n",
        "# Save OOF diagnostics (log and price)\n",
        "oof_mlp_price   = safe_expm1_clip(oof_mlp_log,   lo, hi)\n",
        "oof_ridge_price = safe_expm1_clip(oof_ridge_log, lo, hi)\n",
        "blend_oof_price = safe_expm1_clip(w_mlp*oof_mlp_log + w_ridge*oof_ridge_log, lo, hi)\n",
        "\n",
        "oof_df = pd.DataFrame({\n",
        "    'sample_id': train['sample_id'],\n",
        "    'price_true': y_true_price,\n",
        "    'oof_mlp_log': oof_mlp_log,\n",
        "    'oof_ridge_log': oof_ridge_log,\n",
        "    'oof_blend_log': w_mlp*oof_mlp_log + w_ridge*oof_ridge_log,\n",
        "    'oof_mlp_price': oof_mlp_price,\n",
        "    'oof_ridge_price': oof_ridge_price,\n",
        "    'oof_blend_price': blend_oof_price,\n",
        "})\n",
        "oof_df.to_csv(Path(SUB_DIR)/'oof.csv', index=False)\n"
      ],
      "metadata": {
        "id": "9IQ6PIKrX625",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "816738cb-cae3-4234-e6f7-d4ef5bfa9b2f"
      },
      "id": "9IQ6PIKrX625",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best OOF weights (MLP, Ridge): (np.float64(0.30000000000000004), np.float64(0.7)) OOF SMAPE: 58.27530762327216\n",
            "Saved submission to /content/drive/MyDrive/68e8d1d70b66d_student_resource/student_resource/submissions/test_out.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h0pVbfZVJLah"
      },
      "id": "h0pVbfZVJLah",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}